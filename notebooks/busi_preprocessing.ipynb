{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80bd965e-8dda-4f47-a282-f711860d7249",
   "metadata": {},
   "source": [
    "# Preprocessing BUSI dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe8cd10-8338-4d74-86d1-b192db9d09ee",
   "metadata": {},
   "source": [
    "# Important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa2762d0-7297-495c-b3a8-29becf798d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902c2357-a76a-4c01-b93b-ca4045ea610f",
   "metadata": {},
   "source": [
    "# Preprocessing class with all needed functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "977f7569-4d9f-4a7d-b23a-458d47081a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BUSIPreprocessor:\n",
    "    def __init__(self, data_dir, output_dir, img_size=(224, 224)):\n",
    "        \"\"\"\n",
    "        Initialize the BUSI dataset preprocessor.\n",
    "        \n",
    "        Args:\n",
    "            data_dir: Directory containing the BUSI dataset with benign, malignant, normal folders\n",
    "            output_dir: Directory to save processed images\n",
    "            img_size: Target size for the processed images\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        # Create output directories if they don't exist\n",
    "        for class_name in ['benign', 'malignant', 'normal']:\n",
    "            os.makedirs(os.path.join(output_dir, \"processed_images\", class_name), exist_ok=True)\n",
    "        \n",
    "        # Define class mapping\n",
    "        self.class_mapping = {\n",
    "            'normal': 0,\n",
    "            'benign': 1,\n",
    "            'malignant': 2\n",
    "        }\n",
    "        \n",
    "        \n",
    "    # loading an image and grayscaling it if necessary\n",
    "    def load_image(self, image_path):\n",
    "        \"\"\"Load an image and convert to grayscale if needed.\"\"\"\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Could not load image: {image_path}\")\n",
    "        \n",
    "        # Convert to grayscale if it's a color image\n",
    "        if len(img.shape) == 3:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        return img\n",
    "\n",
    "    \n",
    "\n",
    "    # Annotations removal (letters, texts, drawings) & top left corner annotation removal\n",
    "    def add_corner_triangle_mask(self, img):\n",
    "        \"\"\"\n",
    "        Adds a black triangle mask to the left corner of the image\n",
    "        \"\"\"\n",
    "        height, width = img.shape[:2]\n",
    "        \n",
    "        # Create a copy of the input image\n",
    "        result = img.copy()\n",
    "        \n",
    "        # Define triangle vertices (adjust these coordinates as needed)\n",
    "        # Format: top-left corner, bottom-left corner, and a point to the right\n",
    "        triangle_pts = np.array([[0, 0], [0, height//16], [width//16, 0]], np.int32)\n",
    "        \n",
    "        # Fill the triangle with black (0)\n",
    "        cv2.fillPoly(result, [triangle_pts], 0)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def remove_annotations(self, img):\n",
    "        # Threshold for bright pixels\n",
    "        _, mask = cv2.threshold(img, 220, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "        # Morphological operations to remove thin text/crosses\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "        mask = cv2.dilate(mask, kernel, iterations=1)\n",
    "    \n",
    "        # Inpaint\n",
    "        inpainted = cv2.inpaint(img, mask, inpaintRadius=1, flags=cv2.INPAINT_TELEA)\n",
    "    \n",
    "        return inpainted\n",
    "\n",
    "\n",
    "        \n",
    "    # Filters applied to the BUSI images\n",
    "    def enhance_contrast(self, img):\n",
    "        \"\"\"\n",
    "        Enhance image contrast using adaptive CLAHE,\n",
    "        tuned for ultrasound images.\n",
    "        \"\"\"\n",
    "        if img.dtype != np.uint8:\n",
    "            img = (img * 255).astype(np.uint8)\n",
    "    \n",
    "        # Adaptive CLAHE parameters for ultrasound images\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        enhanced = clahe.apply(img)\n",
    "        return enhanced\n",
    "    \n",
    "    def normalize_image(self, img):\n",
    "        \"\"\"\n",
    "        Normalize image to [0, 1] range.\n",
    "        \"\"\"\n",
    "        img = img.astype(np.float32)\n",
    "        \n",
    "        # Normalize to 0-1 range\n",
    "        img_min = np.min(img)\n",
    "        img_max = np.max(img)\n",
    "        \n",
    "        if img_max > img_min:\n",
    "            img = (img - img_min) / (img_max - img_min)\n",
    "        \n",
    "        return img\n",
    "    \n",
    "\n",
    "\n",
    "    # Resizing to a standard form, to help avoid CNN bias\n",
    "    def resize_with_padding(self, img, target_size):\n",
    "        h, w = img.shape[:2]\n",
    "        target_h, target_w = target_size\n",
    "    \n",
    "        # Compute scale and new size\n",
    "        scale = min(target_w / w, target_h / h)\n",
    "        new_w, new_h = int(w * scale), int(h * scale)\n",
    "    \n",
    "        # Resize while preserving aspect ratio\n",
    "        resized_img = cv2.resize(img, (new_w, new_h))\n",
    "    \n",
    "        # Create a black canvas\n",
    "        padded_img = np.zeros((target_h, target_w), dtype=resized_img.dtype)\n",
    "    \n",
    "        # Compute padding offsets\n",
    "        x_offset = (target_w - new_w) // 2\n",
    "        y_offset = (target_h - new_h) // 2\n",
    "    \n",
    "        # Place the resized image on the canvas\n",
    "        padded_img[y_offset:y_offset+new_h, x_offset:x_offset+new_w] = resized_img\n",
    "    \n",
    "        return padded_img\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    # All steps in processing an image (the order is important for efficiency and reliability)\n",
    "    def process_image(self, image_path, class_name, save=True):\n",
    "        \"\"\"Process a single ultrasound image.\"\"\"\n",
    "    \n",
    "        # Extract the image filename\n",
    "        filename = os.path.basename(image_path)\n",
    "        name_without_ext = os.path.splitext(filename)[0]\n",
    "        \n",
    "        # Load the image\n",
    "        original_img = self.load_image(image_path)\n",
    "\n",
    "        #  Top left corner annotation removal & annotations removal (letters, texts, drawings)\n",
    "        img_left_corner_removed = self.add_corner_triangle_mask(original_img)\n",
    "        img_reduced = self.remove_annotations(img_left_corner_removed)\n",
    "        \n",
    "        # Filters\n",
    "        img_enhanced = self.enhance_contrast(img_reduced) # Enhance contrast\n",
    "        img_normalized = self.normalize_image(img_enhanced) # Normalize the image\n",
    "        \n",
    "        # Resize the image to the target size (224,224) -> useful for the CNN \n",
    "        img_resized = self.resize_with_padding(img_enhanced, self.img_size)\n",
    "        \n",
    "        # Save the processed image\n",
    "        if save:\n",
    "            output_path = os.path.join(self.output_dir, \"processed_images\", class_name, \n",
    "                                     f\"{name_without_ext}_processed.png\")\n",
    "            cv2.imwrite(output_path, img_resized)\n",
    "        \n",
    "        return {\n",
    "            'filename': filename,\n",
    "            'class': class_name,\n",
    "            'processed_image': img_resized,\n",
    "            'normalized_image': self.resize_with_padding((img_normalized * 255).astype(np.uint8), self.img_size)\n",
    "        }\n",
    "\n",
    "    # For mask, only resing is needed    \n",
    "    def process_mask_image(self, mask_path, class_name, save=True):\n",
    "        \"\"\"Process a mask image without any modifications.\"\"\"\n",
    "        # Extract the mask filename\n",
    "        filename = os.path.basename(mask_path)\n",
    "        name_without_ext = os.path.splitext(filename)[0]\n",
    "        \n",
    "        # Load the mask image\n",
    "        mask_img = self.load_image(mask_path)\n",
    "        \n",
    "        # Only resize the mask to match the target size, no other processing\n",
    "        mask_resized = self.resize_with_padding(mask_img, self.img_size)\n",
    "        \n",
    "        # Save the mask image\n",
    "        if save:\n",
    "            output_path = os.path.join(self.output_dir, \"processed_images\", class_name, filename)\n",
    "            cv2.imwrite(output_path, mask_resized)\n",
    "        \n",
    "        return {\n",
    "            'filename': filename,\n",
    "            'class': class_name,\n",
    "            'mask_image': mask_resized\n",
    "        }\n",
    "\n",
    "        \n",
    "    \n",
    "    def get_image_files(self, class_folder):\n",
    "        \"\"\"\n",
    "        Get all image files from a class folder, excluding mask files.\n",
    "        Returns both regular images and their corresponding mask files (if they exist).\n",
    "        \"\"\"\n",
    "        image_extensions = ['*.png', '*.jpg', '*.jpeg', '*.bmp', '*.tiff']\n",
    "        image_files = []\n",
    "        mask_files = []\n",
    "        \n",
    "        for ext in image_extensions:\n",
    "            files = glob.glob(os.path.join(class_folder, ext))\n",
    "            \n",
    "            # Separate regular images and mask images\n",
    "            for f in files:\n",
    "                if '_mask' in os.path.basename(f).lower():\n",
    "                    mask_files.append(f)\n",
    "                else:\n",
    "                    image_files.append(f)\n",
    "        \n",
    "        return image_files, mask_files\n",
    "\n",
    "\n",
    "    def process_all_images(self):\n",
    "        \"\"\"Process all images and their masks in the dataset.\"\"\"\n",
    "        image_results = []\n",
    "        mask_results = []\n",
    "        \n",
    "        for class_name in ['malignant', 'normal', 'benign']:\n",
    "            class_folder = os.path.join(self.data_dir, class_name)\n",
    "            \n",
    "            if not os.path.exists(class_folder):\n",
    "                print(f\"Warning: Folder {class_folder} does not exist. Skipping...\")\n",
    "                continue\n",
    "            \n",
    "            # Get both regular images and mask images\n",
    "            image_files, mask_files = self.get_image_files(class_folder)\n",
    "            print(f\"Found {len(image_files)} images and {len(mask_files)} masks in {class_name} folder\")\n",
    "            \n",
    "            # Process regular images\n",
    "            for image_path in tqdm(image_files, desc=f\"Processing {class_name} images\"):\n",
    "                try:\n",
    "                    result = self.process_image(image_path, class_name)\n",
    "                    image_results.append(result)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {image_path}: {e}\")\n",
    "            \n",
    "            # Process mask images (without modifications)\n",
    "            for mask_path in tqdm(mask_files, desc=f\"Processing {class_name} masks\"):\n",
    "                try:\n",
    "                    result = self.process_mask_image(mask_path, class_name)\n",
    "                    mask_results.append(result)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing mask {mask_path}: {e}\")\n",
    "        \n",
    "        return image_results, mask_results\n",
    "\n",
    "    \n",
    "    def create_dataset(self, use_normalized=True, combine_masks=True):\n",
    "        \"\"\"Create a dataset for training a CNN.\"\"\"\n",
    "        image_data, mask_data = self.process_all_images()\n",
    "    \n",
    "        # Create dictionaries for easy matching - supports multiple masks per image\n",
    "        mask_dict = {}\n",
    "        for mask in mask_data:\n",
    "            # Extract base filename without _mask suffix and any additional suffixes like _1\n",
    "            mask_filename = mask['filename']\n",
    "            # Remove file extension first\n",
    "            base_name = os.path.splitext(mask_filename)[0]\n",
    "            # Remove _mask and any additional suffixes like _1, _2, etc.\n",
    "            if '_mask' in base_name:\n",
    "                base_name = base_name.split('_mask')[0]\n",
    "            \n",
    "            # Store multiple masks per image in a list\n",
    "            if base_name not in mask_dict:\n",
    "                mask_dict[base_name] = []\n",
    "            mask_dict[base_name].append(mask['mask_image'])\n",
    "        \n",
    "        # Create X (images), y (labels), and masks\n",
    "        X = []\n",
    "        y = []\n",
    "        masks = []\n",
    "        filenames = []\n",
    "        \n",
    "        for data in image_data:\n",
    "            # Extract base name from the current image\n",
    "            img_filename = data['filename']\n",
    "            img_base_name = os.path.splitext(img_filename)[0]\n",
    "            \n",
    "            if use_normalized:\n",
    "                X.append(data['normalized_image'])\n",
    "            else:\n",
    "                X.append(data['processed_image'])\n",
    "            \n",
    "            y.append(self.class_mapping[data['class']])\n",
    "            \n",
    "            # Get corresponding masks\n",
    "            if img_base_name in mask_dict:\n",
    "                img_masks = mask_dict[img_base_name]\n",
    "                \n",
    "                if len(img_masks) == 1:\n",
    "                    masks.append(img_masks[0])\n",
    "                elif combine_masks:\n",
    "                    # Combine multiple masks by taking the maximum value at each pixel\n",
    "                    combined_mask = np.maximum.reduce(img_masks)\n",
    "                    masks.append(combined_mask)\n",
    "                    print(f\"Combined {len(img_masks)} masks for {img_filename}\")\n",
    "                else:\n",
    "                    # Just use the first mask if not combining\n",
    "                    masks.append(img_masks[0])\n",
    "                    print(f\"Using first mask out of {len(img_masks)} for {img_filename}\")\n",
    "            else:\n",
    "                # If no mask found, create a blank mask\n",
    "                blank_mask = np.zeros(self.img_size, dtype=np.uint8)\n",
    "                masks.append(blank_mask)\n",
    "                print(f\"Warning: No mask found for {img_filename}, using blank mask\")\n",
    "            \n",
    "            filenames.append(data['filename'])\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        masks = np.array(masks)\n",
    "    \n",
    "        # Add channel dimension if needed (for CNN)\n",
    "        if len(X.shape) == 3:\n",
    "            X = np.expand_dims(X, axis=-1)\n",
    "        if len(masks.shape) == 3:\n",
    "            masks = np.expand_dims(masks, axis=-1)\n",
    "        \n",
    "        print(f\"Dataset shape: {X.shape}\")\n",
    "        print(f\"Masks shape: {masks.shape}\")\n",
    "        print(f\"Labels shape: {y.shape}\")\n",
    "        print(f\"Class distribution: Normal: {np.sum(y == 0)}, Benign: {np.sum(y == 1)}, Malignant: {np.sum(y == 2)}\")\n",
    "        \n",
    "        # Split into training and validation sets\n",
    "        X_train, X_val, y_train, y_val, masks_train, masks_val = train_test_split(\n",
    "            X, y, masks, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        return X_train, X_val, y_train, y_val, masks_train, masks_val, filenames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b053ae-771a-4f00-9805-d20bc3646cb7",
   "metadata": {},
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "610233e2-4719-4e8e-bd1d-329956a4ed17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(X_train, y_train, masks=None):\n",
    "    \"\"\"\n",
    "    Apply data augmentation to the training set (supports optional masks).\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training images\n",
    "        y_train: Training labels (0=Normal, 1=Benign, 2=Malignant)\n",
    "        masks: Optional masks (None if classification only)\n",
    "        \n",
    "    Returns:\n",
    "        Augmented images, labels, (and masks if provided)\n",
    "    \"\"\"\n",
    "    X_augmented, y_augmented = [], []\n",
    "    mask_augmented = [] if masks is not None else None\n",
    "\n",
    "    for i in range(len(X_train)):\n",
    "        img = X_train[i]\n",
    "        label = y_train[i]\n",
    "        mask = masks[i] if masks is not None else None\n",
    "\n",
    "        # Define per-class augmentation strength\n",
    "        if label == 0:   # Normal (minority)\n",
    "            aug_factor = 4\n",
    "        elif label == 2: # Malignant (mid-size)\n",
    "            aug_factor = 3\n",
    "        else:            # Benign (majority)\n",
    "            aug_factor = 1\n",
    "\n",
    "        # Always keep original\n",
    "        X_augmented.append(img)\n",
    "        y_augmented.append(label)\n",
    "        if mask is not None:\n",
    "            mask_augmented.append(mask)\n",
    "\n",
    "        # Augmentations\n",
    "        for _ in range(aug_factor):\n",
    "            aug_type = np.random.choice(['flip', 'rotate', 'zoom', 'noise', 'brightness'])\n",
    "\n",
    "            img_2d = img.squeeze(-1) if img.ndim == 3 and img.shape[-1] == 1 else img\n",
    "            mask_2d = mask.squeeze(-1) if (mask is not None and mask.ndim == 3 and mask.shape[-1] == 1) else mask\n",
    "\n",
    "            if aug_type == 'flip':\n",
    "                aug_img = cv2.flip(img_2d, 1)\n",
    "                aug_mask = cv2.flip(mask_2d, 1) if mask is not None else None\n",
    "\n",
    "            elif aug_type == 'rotate':\n",
    "                angle = np.random.uniform(-15, 15)\n",
    "                aug_img = ndimage.rotate(img_2d, angle, reshape=False)\n",
    "                aug_mask = ndimage.rotate(mask_2d, angle, reshape=False) if mask is not None else None\n",
    "\n",
    "            elif aug_type == 'zoom':\n",
    "                zoom_factor = np.random.uniform(0.9, 1.1)\n",
    "                h, w = img_2d.shape\n",
    "                new_h, new_w = int(h * zoom_factor), int(w * zoom_factor)\n",
    "                aug_img = cv2.resize(img_2d, (new_w, new_h))\n",
    "                aug_mask = cv2.resize(mask_2d, (new_w, new_h)) if mask is not None else None\n",
    "\n",
    "                if zoom_factor < 1.0:  # pad\n",
    "                    pad_h = (h - new_h) // 2\n",
    "                    pad_w = (w - new_w) // 2\n",
    "                    aug_img = cv2.copyMakeBorder(aug_img, pad_h, h - new_h - pad_h,\n",
    "                                                 pad_w, w - new_w - pad_w,\n",
    "                                                 cv2.BORDER_CONSTANT, value=0)\n",
    "                    if mask is not None:\n",
    "                        aug_mask = cv2.copyMakeBorder(aug_mask, pad_h, h - new_h - pad_h,\n",
    "                                                      pad_w, w - new_w - pad_w,\n",
    "                                                      cv2.BORDER_CONSTANT, value=0)\n",
    "                else:  # crop\n",
    "                    start_h = (new_h - h) // 2\n",
    "                    start_w = (new_w - w) // 2\n",
    "                    aug_img = aug_img[start_h:start_h + h, start_w:start_w + w]\n",
    "                    if mask is not None:\n",
    "                        aug_mask = aug_mask[start_h:start_h + h, start_w:start_w + w]\n",
    "\n",
    "            elif aug_type == 'noise':\n",
    "                noise = np.random.normal(0, np.random.uniform(3, 8), img_2d.shape)\n",
    "                aug_img = np.clip(img_2d.astype(np.float32) + noise, 0, 255).astype(np.uint8)\n",
    "                aug_mask = mask_2d\n",
    "\n",
    "            elif aug_type == 'brightness':\n",
    "                factor = np.random.uniform(0.9, 1.1)\n",
    "                aug_img = np.clip(img_2d.astype(np.float32) * factor, 0, 255).astype(np.uint8)\n",
    "                aug_mask = mask_2d\n",
    "\n",
    "            # Restore channel dim\n",
    "            if img.ndim == 3 and img.shape[-1] == 1:\n",
    "                aug_img = np.expand_dims(aug_img, axis=-1)\n",
    "                if aug_mask is not None:\n",
    "                    aug_mask = np.expand_dims(aug_mask, axis=-1)\n",
    "\n",
    "            X_augmented.append(aug_img)\n",
    "            y_augmented.append(label)\n",
    "            if mask is not None:\n",
    "                mask_augmented.append(aug_mask)\n",
    "\n",
    "    if masks is not None:\n",
    "        return np.array(X_augmented), np.array(y_augmented), np.array(mask_augmented)\n",
    "    else:\n",
    "        return np.array(X_augmented), np.array(y_augmented)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d64794-380f-4e2b-9a3a-40d0b5cfeafd",
   "metadata": {},
   "source": [
    "# Creating the instance of the BUSI processing class, applying preprocessing, creating dataset for CNN and applying data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9de2a7cc-88de-40ca-8a47-9f5a9be69568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 210 images and 211 masks in malignant folder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing malignant images: 100%|██████████████████████████████████████████████████| 210/210 [00:02<00:00, 104.93it/s]\n",
      "Processing malignant masks: 100%|███████████████████████████████████████████████████| 211/211 [00:00<00:00, 698.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 133 images and 133 masks in normal folder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing normal images: 100%|██████████████████████████████████████████████████████| 133/133 [00:01<00:00, 90.15it/s]\n",
      "Processing normal masks: 100%|██████████████████████████████████████████████████████| 133/133 [00:00<00:00, 307.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 437 images and 454 masks in benign folder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing benign images: 100%|██████████████████████████████████████████████████████| 437/437 [00:04<00:00, 96.31it/s]\n",
      "Processing benign masks: 100%|██████████████████████████████████████████████████████| 454/454 [00:00<00:00, 654.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined 2 masks for malignant (53).png\n",
      "Combined 2 masks for benign (100).png\n",
      "Combined 2 masks for benign (163).png\n",
      "Combined 2 masks for benign (173).png\n",
      "Combined 2 masks for benign (181).png\n",
      "Combined 3 masks for benign (195).png\n",
      "Combined 2 masks for benign (25).png\n",
      "Combined 2 masks for benign (315).png\n",
      "Combined 2 masks for benign (346).png\n",
      "Combined 2 masks for benign (4).png\n",
      "Combined 2 masks for benign (424).png\n",
      "Combined 2 masks for benign (54).png\n",
      "Combined 2 masks for benign (58).png\n",
      "Combined 2 masks for benign (83).png\n",
      "Combined 2 masks for benign (92).png\n",
      "Combined 2 masks for benign (93).png\n",
      "Combined 2 masks for benign (98).png\n",
      "Dataset shape: (780, 224, 224, 1)\n",
      "Masks shape: (780, 224, 224, 1)\n",
      "Labels shape: (780,)\n",
      "Class distribution: Normal: 133, Benign: 437, Malignant: 210\n",
      "Original training set: (624, 224, 224, 1), (624,), masks: (624, 224, 224, 1)\n",
      "Augmented training set: (1902, 224, 224, 1), (1902,), masks: (1902, 224, 224, 1)\n",
      "Validation set: (156, 224, 224, 1), (156,), masks: (156, 224, 224, 1)\n",
      "Augmented Normal: 530 samples\n",
      "Augmented Benign: 700 samples\n",
      "Augmented Malignant: 672 samples\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"C:/Users/DragosTrandafiri/BreastCancer_CNN/data/raw/busi\"   # benign, malignant, normal folders\n",
    "output_dir = \"C:/Users/DragosTrandafiri/BreastCancer_CNN/data/processed/busi\"\n",
    "\n",
    "# Create preprocessor\n",
    "preprocessor = BUSIPreprocessor(data_dir, output_dir, img_size=(224, 224))\n",
    "\n",
    "# Create dataset\n",
    "X_train, X_val, y_train, y_val, masks_train, masks_val, filenames = preprocessor.create_dataset()\n",
    "\n",
    "# Apply data augmentation with masks\n",
    "X_train_aug, y_train_aug, masks_train_aug = data_augmentation(X_train, y_train, masks=masks_train)\n",
    "\n",
    "print(f\"Original training set: {X_train.shape}, {y_train.shape}, masks: {masks_train.shape}\")\n",
    "print(f\"Augmented training set: {X_train_aug.shape}, {y_train_aug.shape}, masks: {masks_train_aug.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}, {y_val.shape}, masks: {masks_val.shape}\")\n",
    "\n",
    "# Print class distribution\n",
    "unique, counts = np.unique(y_train_aug, return_counts=True)\n",
    "class_names = ['Normal', 'Benign', 'Malignant']\n",
    "for i, (class_idx, count) in enumerate(zip(unique, counts)):\n",
    "    print(f\"Augmented {class_names[class_idx]}: {count} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61072229-b286-4456-a74a-bb559fd3a26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[242]\n",
      "1\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[0]\n",
      "1\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[0]\n",
      "1\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[0]\n",
      "1\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[176]\n",
      "2\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[162]\n",
      "2\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[172]\n",
      "2\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[165]\n",
      "2\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[231]\n",
      "1\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[0]\n",
      "1\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[190]\n",
      "1\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[190]\n",
      "1\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[194]\n",
      "1\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[0]\n",
      "1\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[0]\n",
      "2\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[0]\n",
      "2\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[0]\n",
      "2\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[0]\n",
      "2\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[36]\n",
      "1\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[37]\n",
      "1\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[143]\n",
      "2\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[174]\n",
      "2\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[185]\n",
      "2\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[129]\n",
      "2\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[0]\n",
      "0\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[0]\n",
      "0\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[0]\n",
      "0\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[0]\n",
      "0\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[0]\n",
      "0\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[183]\n",
      "1\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[138]\n",
      "1\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[55]\n",
      "1\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[77]\n",
      "1\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[149]\n",
      "0\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[134]\n",
      "0\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[51]\n",
      "0\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[148]\n",
      "0\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[150]\n",
      "0\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[71]\n",
      "2\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[66]\n",
      "2\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[76]\n",
      "2\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[176]\n",
      "2\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[112]\n",
      "2\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[111]\n",
      "2\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[107]\n",
      "2\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[68]\n",
      "2\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[170]\n",
      "0\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[185]\n",
      "0\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[0]\n",
      "0\n",
      "[0]\n",
      "------------------------\n",
      "                        \n",
      "[145]\n",
      "0\n",
      "[0]\n",
      "------------------------\n",
      "                        \n"
     ]
    }
   ],
   "source": [
    "for i in range (0, 50):\n",
    "    print(X_train_aug[i][22][22])\n",
    "    print(y_train_aug[i])\n",
    "    print(masks_train_aug[i][22][22])\n",
    "    print(\"------------------------\")\n",
    "    print(\"                        \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a9214f6-ac62-4d94-98ae-b55da097f02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented images saved to: C:/Users/DragosTrandafiri/BreastCancer_CNN/data/augmented/busi/X\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Define output directory for augmented images\n",
    "augmented_dir_X = \"C:/Users/DragosTrandafiri/BreastCancer_CNN/data/augmented/busi/X\"\n",
    "augmented_dir_Mask = \"C:/Users/DragosTrandafiri/BreastCancer_CNN/data/augmented/busi/Mask\"\n",
    "\n",
    "# Class names mapping\n",
    "class_names = ['Normal', 'Benign', 'Malignant']\n",
    "\n",
    "# Create directories if they don’t exist\n",
    "for class_name in class_names:\n",
    "    os.makedirs(os.path.join(augmented_dir_X, class_name), exist_ok=True)\n",
    "    os.makedirs(os.path.join(augmented_dir_Mask, class_name), exist_ok=True)\n",
    "\n",
    "# Save images\n",
    "for i, (img_X, img_Mask, label) in enumerate(zip(X_train_aug, masks_train_aug, y_train_aug)):\n",
    "    # Convert from float [0,1] to uint8 [0,255] if needed\n",
    "    if img_X.max() <= 1.0:\n",
    "        img_to_save_X = (img_X * 255).astype(np.uint8)\n",
    "    else:\n",
    "        img_to_save_X = img_X.astype(np.uint8)\n",
    "\n",
    "    if img_Mask.max() <= 1.0:\n",
    "        img_to_save_mask = (img_Mask * 255).astype(np.uint8)\n",
    "    else:\n",
    "        img_to_save_mask = img_Mask.astype(np.uint8)\n",
    "\n",
    "    # Ensure channel order is correct (OpenCV uses BGR)\n",
    "    if img_to_save_X.shape[-1] == 3:  \n",
    "        img_to_save_X = cv2.cvtColor(img_to_save_X, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    if img_to_save_mask.shape[-1] == 3:  \n",
    "        img_to_save_mask = cv2.cvtColor(img_to_save_mask, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "\n",
    "    # Build filename\n",
    "    filename = f\"aug_{i:05d}.png\"\n",
    "    filepath_x = os.path.join(augmented_dir_X, class_names[label], filename)\n",
    "\n",
    "    # Save\n",
    "    cv2.imwrite(filepath_x, img_to_save_X)\n",
    "\n",
    "\n",
    "    filename = f\"aug_mask_{i:05d}.png\"\n",
    "    filepath_mask = os.path.join(augmented_dir_Mask, class_names[label], filename)\n",
    "\n",
    "    # Save\n",
    "    cv2.imwrite(filepath_mask, img_to_save_mask)\n",
    "\n",
    "print(f\"Augmented images saved to: {augmented_dir_X}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec40b3fc-6290-4eee-9e52-636f906c992f",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_dir_X_non_aug = \"C:/Users/DragosTrandafiri/BreastCancer_CNN/data/non_augmented/busi/X\"\n",
    "augmented_dir_Mask_non_aug = \"C:/Users/DragosTrandafiri/BreastCancer_CNN/data/non_augmented/busi/Mask\"\n",
    "\n",
    "# Class names mapping\n",
    "class_names = ['Normal', 'Benign', 'Malignant']\n",
    "\n",
    "# Create directories if they don’t exist\n",
    "for class_name in class_names:\n",
    "    os.makedirs(os.path.join(augmented_dir_X_non_aug, class_name), exist_ok=True)\n",
    "    os.makedirs(os.path.join(augmented_dir_Mask_non_aug, class_name), exist_ok=True)\n",
    "\n",
    "for i, (img_X, img_Mask, label) in enumerate(zip(X_train, masks_train, y_train)):\n",
    "    # Convert from float [0,1] to uint8 [0,255] if needed\n",
    "    if img_X.max() <= 1.0:\n",
    "        img_to_save_X = (img_X * 255).astype(np.uint8)\n",
    "    else:\n",
    "        img_to_save_X = img_X.astype(np.uint8)\n",
    "\n",
    "    if img_Mask.max() <= 1.0:\n",
    "        img_to_save_mask = (img_Mask * 255).astype(np.uint8)\n",
    "    else:\n",
    "        img_to_save_mask = img_Mask.astype(np.uint8)\n",
    "\n",
    "    # Ensure channel order is correct (OpenCV uses BGR)\n",
    "    if img_to_save_X.shape[-1] == 3:  \n",
    "        img_to_save_X = cv2.cvtColor(img_to_save_X, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    if img_to_save_mask.shape[-1] == 3:  \n",
    "        img_to_save_mask = cv2.cvtColor(img_to_save_mask, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "\n",
    "    # Build filename\n",
    "    filename = f\"aug_{i:05d}.png\"\n",
    "    filepath_x = os.path.join(augmented_dir_X_non_aug, class_names[label], filename)\n",
    "\n",
    "    # Save\n",
    "    cv2.imwrite(filepath_x, img_to_save_X)\n",
    "\n",
    "\n",
    "    filename = f\"aug_mask_{i:05d}.png\"\n",
    "    filepath_mask = os.path.join(augmented_dir_Mask_non_aug, class_names[label], filename)\n",
    "\n",
    "    # Save\n",
    "    cv2.imwrite(filepath_mask, img_to_save_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6291427b-b35c-4642-a24d-df88cabcf76f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
