{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80bd965e-8dda-4f47-a282-f711860d7249",
   "metadata": {},
   "source": [
    "# Preprocessing BUSI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "977f7569-4d9f-4a7d-b23a-458d47081a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 210 images and 211 masks in malignant folder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing malignant images: 100%|███████████████████████████████████████████████████| 210/210 [00:02<00:00, 94.16it/s]\n",
      "Processing malignant masks: 100%|███████████████████████████████████████████████████| 211/211 [00:00<00:00, 627.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 133 images and 133 masks in normal folder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing normal images: 100%|██████████████████████████████████████████████████████| 133/133 [00:01<00:00, 85.00it/s]\n",
      "Processing normal masks: 100%|██████████████████████████████████████████████████████| 133/133 [00:00<00:00, 277.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 437 images and 454 masks in benign folder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing benign images: 100%|██████████████████████████████████████████████████████| 437/437 [00:04<00:00, 91.62it/s]\n",
      "Processing benign masks: 100%|██████████████████████████████████████████████████████| 454/454 [00:00<00:00, 610.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (780, 224, 224, 1)\n",
      "Labels shape: (780,)\n",
      "Class distribution: Normal: 133, Benign: 437, Malignant: 210\n",
      "Original training set: (624, 224, 224, 1), (624,)\n",
      "Augmented training set: (2284, 224, 224, 1), (2284,)\n",
      "Validation set: (156, 224, 224, 1), (156,)\n",
      "Augmented Normal: 212 samples\n",
      "Augmented Benign: 1400 samples\n",
      "Augmented Malignant: 672 samples\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "from scipy import ndimage\n",
    "\n",
    "class BUSIPreprocessor:\n",
    "    def __init__(self, data_dir, output_dir, img_size=(224, 224)):\n",
    "        \"\"\"\n",
    "        Initialize the BUSI dataset preprocessor.\n",
    "        \n",
    "        Args:\n",
    "            data_dir: Directory containing the BUSI dataset with benign, malignant, normal folders\n",
    "            output_dir: Directory to save processed images\n",
    "            img_size: Target size for the processed images\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        # Create output directories if they don't exist\n",
    "        for class_name in ['benign', 'malignant', 'normal']:\n",
    "            os.makedirs(os.path.join(output_dir, \"processed_images\", class_name), exist_ok=True)\n",
    "        \n",
    "        # Define class mapping\n",
    "        self.class_mapping = {\n",
    "            'normal': 0,\n",
    "            'benign': 1,\n",
    "            'malignant': 2\n",
    "        }\n",
    "    \n",
    "    def load_image(self, image_path):\n",
    "        \"\"\"Load an image and convert to grayscale if needed.\"\"\"\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Could not load image: {image_path}\")\n",
    "        \n",
    "        # Convert to grayscale if it's a color image\n",
    "        if len(img.shape) == 3:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def enhance_contrast(self, img):\n",
    "        \"\"\"\n",
    "        Enhance image contrast using adaptive CLAHE,\n",
    "        tuned for ultrasound images.\n",
    "        \"\"\"\n",
    "        if img.dtype != np.uint8:\n",
    "            img = (img * 255).astype(np.uint8)\n",
    "    \n",
    "        # Adaptive CLAHE parameters for ultrasound images\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        enhanced = clahe.apply(img)\n",
    "        return enhanced\n",
    "    \n",
    "    # def denoise_image(self, img):\n",
    "    #     \"\"\"\n",
    "    #     Apply denoising to reduce speckle noise common in ultrasound images.\n",
    "    #     \"\"\"\n",
    "    #     # Apply Non-local Means Denoising\n",
    "    #     denoised = cv2.fastNlMeansDenoising(img, None, 3, 7, 21)\n",
    "    #     return denoised\n",
    "    \n",
    "    def normalize_image(self, img):\n",
    "        \"\"\"\n",
    "        Normalize image to [0, 1] range.\n",
    "        \"\"\"\n",
    "        img = img.astype(np.float32)\n",
    "        \n",
    "        # Normalize to 0-1 range\n",
    "        img_min = np.min(img)\n",
    "        img_max = np.max(img)\n",
    "        \n",
    "        if img_max > img_min:\n",
    "            img = (img - img_min) / (img_max - img_min)\n",
    "        \n",
    "        return img\n",
    "    \n",
    "\n",
    "\n",
    "    def soften_annotations(self, img):\n",
    "        \"\"\"\n",
    "        Alternative: Morphological closing to remove thin bright text/lines.\n",
    "        Works better if you don't want to risk losing edges.\n",
    "        \"\"\"\n",
    "        # Threshold for bright pixels\n",
    "        _, mask = cv2.threshold(img, 220, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "        # Morphological operations to remove thin text/crosses\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "        mask = cv2.dilate(mask, kernel, iterations=1)\n",
    "    \n",
    "        # Inpaint\n",
    "        inpainted = cv2.inpaint(img, mask, inpaintRadius=1, flags=cv2.INPAINT_TELEA)\n",
    "    \n",
    "        return inpainted\n",
    "\n",
    "\n",
    "\n",
    "    def resize_with_padding(self, img, target_size):\n",
    "        h, w = img.shape[:2]\n",
    "        target_h, target_w = target_size\n",
    "    \n",
    "        # Compute scale and new size\n",
    "        scale = min(target_w / w, target_h / h)\n",
    "        new_w, new_h = int(w * scale), int(h * scale)\n",
    "    \n",
    "        # Resize while preserving aspect ratio\n",
    "        resized_img = cv2.resize(img, (new_w, new_h))\n",
    "    \n",
    "        # Create a black canvas\n",
    "        padded_img = np.zeros((target_h, target_w), dtype=resized_img.dtype)\n",
    "    \n",
    "        # Compute padding offsets\n",
    "        x_offset = (target_w - new_w) // 2\n",
    "        y_offset = (target_h - new_h) // 2\n",
    "    \n",
    "        # Place the resized image on the canvas\n",
    "        padded_img[y_offset:y_offset+new_h, x_offset:x_offset+new_w] = resized_img\n",
    "    \n",
    "        return padded_img\n",
    "\n",
    "    def add_corner_triangle_mask(self, img):\n",
    "        \"\"\"\n",
    "        Adds a black triangle mask to the left corner of the image\n",
    "        \"\"\"\n",
    "        height, width = img.shape[:2]\n",
    "        \n",
    "        # Create a copy of the input image\n",
    "        result = img.copy()\n",
    "        \n",
    "        # Define triangle vertices (adjust these coordinates as needed)\n",
    "        # Format: top-left corner, bottom-left corner, and a point to the right\n",
    "        triangle_pts = np.array([[0, 0], [0, height//16], [width//16, 0]], np.int32)\n",
    "        \n",
    "        # Fill the triangle with black (0)\n",
    "        cv2.fillPoly(result, [triangle_pts], 0)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    \n",
    "    def process_image(self, image_path, class_name, save=True):\n",
    "        \"\"\"Process a single ultrasound image.\"\"\"\n",
    "        # Extract the image filename\n",
    "        filename = os.path.basename(image_path)\n",
    "        name_without_ext = os.path.splitext(filename)[0]\n",
    "        \n",
    "        # Load the image\n",
    "        original_img = self.load_image(image_path)\n",
    "\n",
    "        img_left_corner_removed = self.add_corner_triangle_mask(original_img)\n",
    "\n",
    "        img_reduced = self.soften_annotations(img_left_corner_removed)\n",
    "        \n",
    "        \n",
    "        # Enhance contrast\n",
    "        img_enhanced = self.enhance_contrast(img_reduced)\n",
    "        \n",
    "        # Normalize the image\n",
    "        img_normalized = self.normalize_image(img_enhanced)\n",
    "        \n",
    "        # Resize the image to the target size\n",
    "        img_resized = self.resize_with_padding(img_enhanced, self.img_size)\n",
    "        \n",
    "        # Save the processed image\n",
    "        if save:\n",
    "            output_path = os.path.join(self.output_dir, \"processed_images\", class_name, \n",
    "                                     f\"{name_without_ext}_processed.png\")\n",
    "            cv2.imwrite(output_path, img_resized)\n",
    "        \n",
    "        return {\n",
    "            'filename': filename,\n",
    "            'class': class_name,\n",
    "            'processed_image': img_resized,\n",
    "            'normalized_image': self.resize_with_padding((img_normalized * 255).astype(np.uint8), self.img_size)\n",
    "        }\n",
    "    \n",
    "    def get_image_files(self, class_folder):\n",
    "        \"\"\"\n",
    "        Get all image files from a class folder, excluding mask files.\n",
    "        Returns both regular images and their corresponding mask files (if they exist).\n",
    "        \"\"\"\n",
    "        image_extensions = ['*.png', '*.jpg', '*.jpeg', '*.bmp', '*.tiff']\n",
    "        image_files = []\n",
    "        mask_files = []\n",
    "        \n",
    "        for ext in image_extensions:\n",
    "            files = glob.glob(os.path.join(class_folder, ext))\n",
    "            \n",
    "            # Separate regular images and mask images\n",
    "            for f in files:\n",
    "                if '_mask' in os.path.basename(f).lower():\n",
    "                    mask_files.append(f)\n",
    "                else:\n",
    "                    image_files.append(f)\n",
    "        \n",
    "        return image_files, mask_files\n",
    "\n",
    "    def process_mask_image(self, mask_path, class_name, save=True):\n",
    "        \"\"\"Process a mask image without any modifications.\"\"\"\n",
    "        # Extract the mask filename\n",
    "        filename = os.path.basename(mask_path)\n",
    "        name_without_ext = os.path.splitext(filename)[0]\n",
    "        \n",
    "        # Load the mask image\n",
    "        mask_img = self.load_image(mask_path)\n",
    "        \n",
    "        # Only resize the mask to match the target size, no other processing\n",
    "        mask_resized = self.resize_with_padding(mask_img, self.img_size)\n",
    "        \n",
    "        # Save the mask image\n",
    "        if save:\n",
    "            output_path = os.path.join(self.output_dir, \"processed_images\", class_name, filename)\n",
    "            cv2.imwrite(output_path, mask_resized)\n",
    "        \n",
    "        return {\n",
    "            'filename': filename,\n",
    "            'class': class_name,\n",
    "            'mask_image': mask_resized\n",
    "        }\n",
    "\n",
    "    def process_all_images(self):\n",
    "        \"\"\"Process all images and their masks in the dataset.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for class_name in ['malignant', 'normal', 'benign']:\n",
    "            class_folder = os.path.join(self.data_dir, class_name)\n",
    "            \n",
    "            if not os.path.exists(class_folder):\n",
    "                print(f\"Warning: Folder {class_folder} does not exist. Skipping...\")\n",
    "                continue\n",
    "            \n",
    "            # Get both regular images and mask images\n",
    "            image_files, mask_files = self.get_image_files(class_folder)\n",
    "            print(f\"Found {len(image_files)} images and {len(mask_files)} masks in {class_name} folder\")\n",
    "            \n",
    "            # Process regular images\n",
    "            for image_path in tqdm(image_files, desc=f\"Processing {class_name} images\"):\n",
    "                try:\n",
    "                    result = self.process_image(image_path, class_name)\n",
    "                    results.append(result)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {image_path}: {e}\")\n",
    "            \n",
    "            # Process mask images (without modifications)\n",
    "            for mask_path in tqdm(mask_files, desc=f\"Processing {class_name} masks\"):\n",
    "                try:\n",
    "                    result = self.process_mask_image(mask_path, class_name)\n",
    "                    # We don't add mask results to the main results list\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing mask {mask_path}: {e}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "    \n",
    "    def create_dataset(self, use_normalized=True):\n",
    "        \"\"\"Create a dataset for training a CNN.\"\"\"\n",
    "        processed_data = self.process_all_images()\n",
    "        \n",
    "        # Create X (images) and y (labels)\n",
    "        X = []\n",
    "        y = []\n",
    "        filenames = []\n",
    "        \n",
    "        for data in processed_data:\n",
    "            if use_normalized:\n",
    "                X.append(data['normalized_image'])\n",
    "            else:\n",
    "                X.append(data['processed_image'])\n",
    "            \n",
    "            y.append(self.class_mapping[data['class']])\n",
    "            filenames.append(data['filename'])\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        # Add channel dimension if needed (for CNN)\n",
    "        if len(X.shape) == 3:\n",
    "            X = np.expand_dims(X, axis=-1)\n",
    "        \n",
    "        print(f\"Dataset shape: {X.shape}\")\n",
    "        print(f\"Labels shape: {y.shape}\")\n",
    "        print(f\"Class distribution: Normal: {np.sum(y == 0)}, Benign: {np.sum(y == 1)}, Malignant: {np.sum(y == 2)}\")\n",
    "        \n",
    "        # Split into training and validation sets\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        return X_train, X_val, y_train, y_val, filenames\n",
    "\n",
    "\n",
    "def data_augmentation(X_train, y_train, augmentation_factor=2):\n",
    "    \"\"\"\n",
    "    Apply data augmentation to the training set.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training images\n",
    "        y_train: Training labels\n",
    "        augmentation_factor: How many times to augment the dataset\n",
    "        \n",
    "    Returns:\n",
    "        Augmented training set\n",
    "    \"\"\"\n",
    "    X_augmented = []\n",
    "    y_augmented = []\n",
    "    \n",
    "    # Add original data\n",
    "    X_augmented.extend(X_train)\n",
    "    y_augmented.extend(y_train)\n",
    "    \n",
    "    for i in range(len(X_train)):\n",
    "        img = X_train[i]\n",
    "        label = y_train[i]\n",
    "        \n",
    "        # Remove channel dimension for processing if present\n",
    "        if len(img.shape) == 3 and img.shape[-1] == 1:\n",
    "            img_2d = img.squeeze(-1)\n",
    "        else:\n",
    "            img_2d = img\n",
    "        \n",
    "        # Augment abnormal cases (benign and malignant) more\n",
    "        aug_factor = augmentation_factor if label > 0 else 1\n",
    "        \n",
    "        for _ in range(aug_factor):\n",
    "            # Randomly choose augmentation techniques\n",
    "            augmentation_type = np.random.choice(['flip', 'rotate', 'zoom', 'noise', 'brightness'])\n",
    "            \n",
    "            if augmentation_type == 'flip':\n",
    "                # Horizontal flip\n",
    "                augmented_img = cv2.flip(img_2d, 1)\n",
    "            \n",
    "            elif augmentation_type == 'rotate':\n",
    "                # Random rotation between -15 and 15 degrees\n",
    "                angle = np.random.uniform(-15, 15)\n",
    "                augmented_img = ndimage.rotate(img_2d, angle, reshape=False)\n",
    "            \n",
    "            elif augmentation_type == 'zoom':\n",
    "                # Random zoom between 0.9 and 1.1\n",
    "                zoom_factor = np.random.uniform(0.9, 1.1)\n",
    "                h, w = img_2d.shape[:2]\n",
    "                \n",
    "                # Calculate new dimensions\n",
    "                new_h, new_w = int(h * zoom_factor), int(w * zoom_factor)\n",
    "                \n",
    "                if zoom_factor < 1.0:  # Zoom out\n",
    "                    augmented_img = cv2.resize(img_2d, (new_w, new_h))\n",
    "                    # Pad to original size\n",
    "                    pad_h = (h - new_h) // 2\n",
    "                    pad_w = (w - new_w) // 2\n",
    "                    augmented_img = cv2.copyMakeBorder(\n",
    "                        augmented_img, pad_h, h - new_h - pad_h, pad_w, w - new_w - pad_w,\n",
    "                        cv2.BORDER_CONSTANT, value=0\n",
    "                    )\n",
    "                else:  # Zoom in\n",
    "                    augmented_img = cv2.resize(img_2d, (new_w, new_h))\n",
    "                    # Crop center to original size\n",
    "                    start_h = (new_h - h) // 2\n",
    "                    start_w = (new_w - w) // 2\n",
    "                    augmented_img = augmented_img[start_h:start_h + h, start_w:start_w + w]\n",
    "            \n",
    "            elif augmentation_type == 'noise':\n",
    "                # Add Gaussian noise\n",
    "                mean = 0\n",
    "                stddev = np.random.uniform(5, 15)  # For uint8 images\n",
    "                noise = np.random.normal(mean, stddev, img_2d.shape).astype(np.float32)\n",
    "                augmented_img = np.clip(img_2d.astype(np.float32) + noise, 0, 255).astype(np.uint8)\n",
    "            \n",
    "            elif augmentation_type == 'brightness':\n",
    "                # Random brightness adjustment\n",
    "                brightness_factor = np.random.uniform(0.8, 1.2)\n",
    "                augmented_img = np.clip(img_2d.astype(np.float32) * brightness_factor, 0, 255).astype(np.uint8)\n",
    "            \n",
    "            # Add channel dimension back if needed\n",
    "            if len(X_train.shape) == 4 and X_train.shape[-1] == 1:\n",
    "                augmented_img = np.expand_dims(augmented_img, axis=-1)\n",
    "            \n",
    "            X_augmented.append(augmented_img)\n",
    "            y_augmented.append(label)\n",
    "    \n",
    "    return np.array(X_augmented), np.array(y_augmented)\n",
    "\n",
    "\n",
    "# Define paths - update these to match your BUSI dataset location\n",
    "data_dir = \"C:/Users/DragosTrandafiri/BreastCancer_CNN/data/raw/busi\"   # This should contain benign, malignant, normal folders\n",
    "output_dir = \"C:/Users/DragosTrandafiri/BreastCancer_CNN/data/processed/busi\"\n",
    "\n",
    "# Create preprocessor\n",
    "preprocessor = BUSIPreprocessor(data_dir, output_dir, img_size=(224, 224))\n",
    "\n",
    "# Create dataset\n",
    "X_train, X_val, y_train, y_val, filenames = preprocessor.create_dataset()\n",
    "\n",
    "# Apply data augmentation\n",
    "X_train_aug, y_train_aug = data_augmentation(X_train, y_train, augmentation_factor=3)\n",
    "\n",
    "print(f\"Original training set: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Augmented training set: {X_train_aug.shape}, {y_train_aug.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}, {y_val.shape}\")\n",
    "\n",
    "# Print class distribution\n",
    "unique, counts = np.unique(y_train_aug, return_counts=True)\n",
    "class_names = ['Normal', 'Benign', 'Malignant']\n",
    "for i, (class_idx, count) in enumerate(zip(unique, counts)):\n",
    "    print(f\"Augmented {class_names[class_idx]}: {count} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae45c2ef-3712-474d-b83c-0d99d38bd5db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
