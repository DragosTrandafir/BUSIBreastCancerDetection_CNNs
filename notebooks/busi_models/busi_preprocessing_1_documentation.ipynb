{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "408087ca-a0f5-48ac-9c0b-82448960c63f",
   "metadata": {},
   "source": [
    "# Explaining functions and parameters from busi_preprocessing_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f99325-76f6-4f81-aca8-e95901615390",
   "metadata": {},
   "source": [
    "## def remove_annotations(self, img)\n",
    "\n",
    "### 1. _, mask = cv2.threshold(img, 220, 255, cv2.THRESH_BINARY) \n",
    "  - applies binary thresholding to detect very bright pixels (e.g., white annotations).\n",
    "\n",
    "- Parameters:\n",
    "\n",
    "img: the input grayscale image.\n",
    "\n",
    "220: threshold value. Pixels with value >220 will be set to 255(white), others to 0(black).\n",
    "\n",
    "255: max value to assign to thresholded pixels.\n",
    "\n",
    "cv2.THRESH_BINARY: sets all pixels above the threshold to 255, the rest to 0.\n",
    "\n",
    "- Output:\n",
    "\n",
    "_: the actual threshold used(not needed here).\n",
    "\n",
    "mask: a binary mask where white regions (255) indicate possible annotations.\n",
    "\n",
    "- Effect:\n",
    "\n",
    "Creates a binary map of potential annotation regions.\n",
    "\n",
    "### 2. kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "\n",
    "- defines a morphological kernel for processing the mask.\n",
    "\n",
    "- Parameters:\n",
    "\n",
    "cv2.MORPH_RECT: defines a rectangular kernel shape.\n",
    "\n",
    "(3, 3): kernel size: a 3×3 square\n",
    "\n",
    "- Effect:\n",
    "\n",
    "The kernel will be used in dilation to expand white regions (i.e., annotation marks) a bit, making them more detectable for inpainting.\n",
    "\n",
    "### 3. mask = cv2.dilate(mask, kernel, iterations=1)\n",
    "\n",
    "- Applies dilation to the binary mask\n",
    "\n",
    "- Parameters:\n",
    "\n",
    "mask: input binary mask from thresholding\n",
    "\n",
    "kernel: the structuring element from the previous step\n",
    "\n",
    "iterations=1: how many times to apply dilation.\n",
    "\n",
    "- Effect:\n",
    "\n",
    "Slightly enlarges the white regions, allowing thin text or lines (like crosses) to be more effectively covered and removed.\n",
    "\n",
    "### 4. inpainted = cv2.inpaint(img, mask, inpaintRadius=1, flags=cv2.INPAINT_TELEA)\n",
    "\n",
    "- Performs image inpainting, i.e., reconstructs regions defined by the mask using surrounding pixels.\n",
    "\n",
    "- Parameters:\n",
    "\n",
    "img: original grayscale image\n",
    "\n",
    "mask: binary mask — white areas are inpainted\n",
    "\n",
    "inpaintRadius=1: defines how far out to look around each pixel to reconstruct it (small values (like 1–3) are usually enough for thin annotations)\n",
    "\n",
    "flags=cv2.INPAINT_TELEA: uses the TELEA algorithm, a fast inpainting method good for small, sharp features\n",
    "\n",
    "- Effect:\n",
    "\n",
    "Reconstructs annotated areas using nearby pixel information to make them \"disappear\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c24a73-8554-4b03-beaa-466ed8a48dff",
   "metadata": {},
   "source": [
    "## def enhance_contrast(self, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e806ba4-fe98-4741-a482-401a40b9c20f",
   "metadata": {},
   "source": [
    "### 1. img = (img * 255).astype(np.uint8)\n",
    "If the image is in float format (e.g., values in range [0, 1]), it’s scaled to [0, 255] and converted to 8-bit unsigned integers; CLAHE in OpenCV only works with uint8 images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a68ee49-c3fe-4bda-a465-3b18a404e750",
   "metadata": {},
   "source": [
    "### 2. clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "\n",
    "- This creates a CLAHE object with specific parameters.\n",
    "\n",
    "*CLAHE = Contrast Limited Adaptive Histogram Equalization* - a localized form of histogram equalization — great for preserving detail and avoiding over-amplification of noise in ultrasound images.\n",
    "\n",
    "- Parameters:\n",
    "\n",
    "      -- clipLimit=2.0:\n",
    "\n",
    "Prevents noise amplification by limiting the contrast enhancement in very uniform regions.\n",
    "\n",
    "Typical values: 1.0–4.0; smaller = less enhancement.\n",
    "\n",
    "     -- tileGridSize=(8, 8):\n",
    "\n",
    "Image is divided into 8×8 tiles (regions).\n",
    "\n",
    "CLAHE is applied separately to each tile → preserves local contrast better than global equalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb0d209-7009-44fa-81d7-f6a25707c76a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
